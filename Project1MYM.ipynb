{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS559 - F20 Project #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Desciption\n",
    "You are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column.\n",
    "\n",
    "The task is to predict the value of `target` column in the test set using either **Logistic Regression** and **SVM**. You are welcome to use **regularizaiton**. \n",
    "\n",
    "## File descriptions\n",
    "- train.csv - the training set (202 columns)\n",
    "\n",
    "- test.csv - the test set. The test set contains some rows which are not included in scoring.\n",
    "\n",
    "## Rules\n",
    "- The data does not have specific column names. Therefore, you will not know what data is about. \n",
    "- However, you still can do classicaition problem without clustering the training set. **No unsupervised learning techniques in this project**. \n",
    "- There are 202 columns. This means that the key of high accuracy comes from **EDA** and **feature enegineering**. \n",
    "- There are no rules on EDA and Feature Engineering. \n",
    "- On your model, make sure you can reduce the columns at the most of 25%. If we use all columns, we may have high computational cost and getting into bias-variance tradeoff and underfit vs. overfit situations. \n",
    "- The project is out of 100. \n",
    "    - 50 points will come from your EDA and any pre-processing work. \n",
    "    - 30 points will come from your model: Accuracy + overcoming any ML challenges. \n",
    "    - 10 points will come from in-class competition. \n",
    "        - Ranking the accuracy with less features. \n",
    "    - 10 points will come from a report describing your work flow and model evaluations.\n",
    "        - must be submitted in different file (e.g., pdf, docx). \n",
    "        \n",
    "## Recommand Before-Preprocessing\n",
    "- You can split the set from the data distribution. \n",
    "- You can make multiple new data frames by randomly selecting columns. \n",
    "- You can do similar by rows. \n",
    "\n",
    "## Recommand Before-training model\n",
    "- Make sure to delete features from supportive reasons. \n",
    "\n",
    "Proejct DUE: 10/23/2020 Friday 11:59 PM. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#Read in dataset\n",
    "dataset = pd.read_csv(\"Train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate features and target, drop unnecessary ID_code column\n",
    "features = dataset.drop(\"target\", axis = 1)\n",
    "features = dataset.drop(\"ID_code\", axis = 1)\n",
    "target = dataset[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess best features\n",
    "correlations = features.corrwith(target)\n",
    "correlations = correlations.reindex(correlations.abs().sort_values().index)\n",
    "\n",
    "\n",
    "#Use correlations to select the 25% maximally corelated features\n",
    "selected_features = []\n",
    "for row in range(3*len(correlations.index)//4, len(correlations.index)):\n",
    "    selected_features += [correlations.index[row]]\n",
    "\n",
    "#Make dataframe of selected features\n",
    "new_features = selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Personal implementation of LogisticRegression, allowing for learning rate to be modified\n",
    "class LogisticRegressionPersonal:\n",
    "    def __init__(self, lr = 0.01, n = 100000, features_list_index = None):\n",
    "        self.lr = lr\n",
    "        self.num_iter = n\n",
    "        self.features_list_index = features_list_index\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.delete(X.to_numpy(), 0, 1)\n",
    "        y = y.to_numpy()\n",
    "\n",
    "        # weights initialization\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            try:\n",
    "                z = np.array(np.dot(X, self.theta), dtype = np.float64)\n",
    "                h = self.sigmoid(z)\n",
    "                gradient = np.array(np.dot(X.T, (h - y)) / y.size, dtype = np.float64)\n",
    "                self.theta -= self.lr * gradient\n",
    "            except:\n",
    "                print(X, self.theta)\n",
    "        \n",
    "    def predict_prob(self, X):\n",
    "        z = np.array(np.dot(X, self.theta), dtype = np.float64)\n",
    "        return self.sigmoid(np.array(np.dot(X, self.theta), dtype = np.float64))\n",
    "    \n",
    "    def predict(self, X, threshold = .5):\n",
    "        X = np.delete(X.to_numpy(), 0, 1)\n",
    "        return self.predict_prob(X) >= threshold\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Learning Rate: \" + str(self.lr) + '\\n' + \"Number of Iterations: \" + str(self.num_iter) +'\\n' + \"Features list index: \" + str(self.features_list_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8a624b7317ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#List to hold different feature lists, starting with the one that was selected using EDA to find the most correlated features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mfeatures_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#Select random features in batches and create feature lists with this, to compare to the correlations based one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_features' is not defined"
     ]
    }
   ],
   "source": [
    "#List of learning rates to create varied models of LogisticRegressionPersonal\n",
    "learning_rates = [.003, .005, .01]\n",
    "\n",
    "#List of iteration limits to create varied models for both implementations of Logistic Regression\n",
    "num_iterations = [500, 1000, 5000]\n",
    "\n",
    "#List to hold different feature lists, starting with the one that was selected using EDA to find the most correlated features\n",
    "features_list = [new_features]\n",
    "\n",
    "#Select random features in batches and create feature lists with this, to compare to the correlations based one\n",
    "for i in range(2):\n",
    "    indices = random.sample(range(1, 200), 50)\n",
    "    selected_features = []\n",
    "    for row in indices:\n",
    "        selected_features += [features.iloc[:, row].name]\n",
    "    features_list += [selected_features]\n",
    "\n",
    "#List to store the top 10 performing models\n",
    "best_lr_models = []\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create varied models for LogisticRegressionPersonal\n",
    "for i in range(len(features_list)):\n",
    "    temp = []\n",
    "    for rate in learning_rates:\n",
    "        temp2 = []\n",
    "        for num in num_iterations:\n",
    "            print(\"Learning Rate: \" + str(rate))\n",
    "            print(\"Number of Iterations: \" + str(num))\n",
    "            print(\"Features list index: \" + str(i))\n",
    "            lr = LogisticRegressionPersonal(rate, num, i)\n",
    "            print(\"Fitting Model...\")\n",
    "            lr.fit(features[features_list[i]], target)\n",
    "            preds = lr.predict(features[features_list[i]])\n",
    "            final = list(zip(list(preds), list(target.to_numpy())))\n",
    "            accuracy = 0\n",
    "            for pair in final:\n",
    "                if pair[0] == pair[1]:\n",
    "                    accuracy += 1\n",
    "            perc = accuracy/len(target)\n",
    "            print(\"Model Accuracy: \" + str(perc) + '\\n')\n",
    "            best_lr_models += [(\"Personal\", lr, perc)]\n",
    "            best_lr_models.sort(key = lambda x: x[2])\n",
    "            if len(best_lr_models) > 10:\n",
    "                best_lr_models = best_lr_models[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-37808194a6e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Create varied models for Sklearn's Logistic Regression models that use Stochastic Average Gradients which do not allow for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#user input learning rates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Learning Rate: N/A\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of Iterations: N/A\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features_list' is not defined"
     ]
    }
   ],
   "source": [
    "#Create varied models for Sklearn's Logistic Regression models that use Stochastic Average Gradients which do not allow for \n",
    "#user input learning rates\n",
    "for i in range(len(features_list)):\n",
    "    print(\"Learning Rate: N/A\")\n",
    "    print(\"Number of Iterations: N/A\")\n",
    "    print(\"Features list index: \" + str(i))\n",
    "    lr = LogisticRegression(max_iter = num)\n",
    "    print(\"Fitting Model...\")\n",
    "    lr.fit(features[features_list[i]], target.values)\n",
    "    preds = lr.predict(features[features_list[i]])\n",
    "    final = list(zip(list(preds), list(target.values)))\n",
    "    accuracy = 0\n",
    "    for pair in final:\n",
    "        if pair[0] == pair[1]:\n",
    "            accuracy += 1\n",
    "    print(\"Model Accuracy: \" + str(accuracy/len(target)) + '\\n')\n",
    "    perc = accuracy/len(target)\n",
    "    best_lr_models += [(\"Sklearn\", lr, perc)]\n",
    "    best_lr_models.sort(key = lambda x: x[2])\n",
    "    if len(best_lr_models) > 10:\n",
    "        best_lr_models = best_lr_models[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort top ten Logistic Regression models by accuracy with training set, allowing for competition between them.\n",
    "best_lr_models.sort(key = lambda x: x[2])\n",
    "for pair in best_lr_models:\n",
    "    print(pair[0])\n",
    "    print(pair[1])\n",
    "    print(\"Accuracy: \" + str(pair[2]) + '\\n')\n",
    "    print(\"---------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8997\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
